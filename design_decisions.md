# Design Decisions - Технические решения

## Обзор решения

Программа использует алгоритм группировки анаграмм на основе сортировки символов с применением хеш-таблицы (словаря Python) для эффективного поиска групп.

## Выбор алгоритма

### Рассмотренные варианты:

1. **Сортировка символов** (выбран)
2. Подсчет частоты символов
3. Произведение простых чисел

### Обоснование выбора сортировки символов:

**Преимущества:**
- **Простота реализации**: `''.join(sorted(word))`
- **Надежность**: отсутствие коллизий
- **Производительность**: O(n × m log m) где n - слова, m - длина слова
- **Читаемость кода**: легко понять и поддерживать

**Недостатки:**
- Сортировка каждого слова добавляет O(m log m) операций
- Для очень длинных слов может быть медленнее подсчета частот

**Вердикт**: Для большинства практических задач сортировка символов оптимальна по соотношению простота/производительность.

## Архитектура программы

### Модульная структура:

```python
def read_words_from_file(filename)  # Чтение и очистка данных
def group_anagrams(words)           # Основной алгоритм группировки  
if __name__ == "__main__":          # Точка входа
```

### Принципы дизайна:

1. **Разделение ответственности**: каждая функция решает одну задачу
2. **Обработка ошибок**: четкие исключения для разных типов проблем
3. **Читаемость**: простая логика без излишних абстракций
4. **Тестируемость**: функции легко тестировать изолированно

## Структуры данных

### Основная структура - Python dict:

```python
anagram_groups = {
    "acer": ["race", "care", "acre"],
    "act": ["cat", "act"],
    "eert": ["tree"]
}
```

**Обоснование:**
- **Быстрый поиск**: O(1) в среднем случае
- **Автоматическая группировка**: одинаковые ключи объединяют слова
- **Память**: эффективное хранение без дублирования

## Обработка данных

### Очистка входных данных:

```python
if not word:
    continue        
if word.isalpha():
    words.append(word)
```

**Решения:**
- **Приведение к нижнему регистру**: унификация "Cat" и "cat"
- **Фильтрация**: только буквы, исключение чисел и символов
- **Пропуск пустых строк**: избежание ошибок в обработке

### Обработка ошибок:

- **FileNotFoundError**: информативное сообщение + re-raise
- **IOError**: логирование проблем доступа + re-raise
- **Стратегия**: показать пользователю проблему, но не скрывать исключение

## Масштабируемость

### Для 10 миллионов слов:

**Текущее решение подходит:**
- **Память**: ~1GB для 10M слов (средняя длина 10 символов)
- **Время**: ~30 секунд на современном ПК
- **Узкое место**: сортировка символов в каждом слове

**Возможные оптимизации:**
- Чтение файла чанками вместо построчного
- Использование более быстрых алгоритмов сортировки для коротких строк

### Для 100 миллиардов слов:

**Кардинальные изменения:**

1. **Распределенная обработка:**
   ```
   Map: word → (sorted_key, word)
   Reduce: group by sorted_key
   ```

2. **Потоковая обработка:**
   - Разбиение файла на части
   - Обработка чанками по 100MB
   - Слияние результатов

3. **Внешняя сортировка:**
   - Использование дискового пространства
   - Библиотеки типа Dask для out-of-core вычислений

4. **База данных:**
   ```sql
   CREATE INDEX ON words(sorted_chars);
   SELECT * FROM words ORDER BY sorted_chars;
   ```

## Альтернативные реализации

### Подсчет частоты символов:

```python
from collections import Counter

def create_key(word):
    return tuple(sorted(Counter(word).items()))
```

**Когда лучше**: для очень длинных слов (>50 символов)

### Использование внешних библиотек:

**Pandas для больших данных:**
```python
import pandas as pd
df['key'] = df['word'].apply(lambda x: ''.join(sorted(x)))
groups = df.groupby('key')['word'].apply(list)
```

**NumPy для числовых операций**: не подходит для строковых данных

## Выводы

Выбранное решение оптимально для задач до 10M слов благодаря:
- Простоте понимания и поддержки
- Хорошей производительности на практических данных  
- Возможности легкого профилирования и оптимизации
- Отсутствию внешних зависимостей

Для масштабирования до миллиардов записей потребуется переход к распределенным вычислениям и потоковой обработке данных.